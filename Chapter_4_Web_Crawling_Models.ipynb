{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 4: Web Crawling Models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOPW4EqKhZbl8qY5IxuawTO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajmbarron/web_scraping_with_python-/blob/main/Chapter_4_Web_Crawling_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPZGDMSjbNZ-"
      },
      "source": [
        "#Functions Module#\n",
        "\n",
        "*Objective: This module is intended to scrap specific websites*\n",
        "\n",
        "\n",
        "\n",
        "***Structure:***\n",
        "*   **Defining class Content**:\n",
        "\n",
        "    With this class we get an url instance characteristics\n",
        "    as url name, title and body.\n",
        "\n",
        "\n",
        "*   ****\n",
        "\n",
        "*   **Defining function to request page and return html**:\n",
        "\n",
        "    getPage() request an url and transforms it to a \n",
        "    BeautifulSoup Object, which would represent\n",
        "    the html source code for an specific url.\n",
        "\n",
        "*   ****\n",
        "\n",
        "*  **Defining functions to scrape based on Content instance:**\n",
        "\n",
        "   As we already defined a class called Content, we can now\n",
        "   get all characterizable elements for this class inside a function.\n",
        "\n",
        "   This functions have specific line tasks to extract all \n",
        "   characterizable elements for Content instance.\n",
        "\n",
        "   Also we use getPage() inside each one to request the pages\n",
        "   as BeautifulSoup classes. \n",
        "\n",
        "   Every parsing function does the same:\n",
        "\n",
        "   * Selects the title element and extracts the text for the title.\n",
        "   * Selects the main content of the article.\n",
        "   * Selects other content items as needed.\n",
        "   * Returns a Content object instantiated witht the strings found previously.\n",
        "\n",
        "   * ****\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnXZT1zvFUDO"
      },
      "source": [
        "################### importing required libraries #############\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "##############################################################\n",
        "# Defining url as a class content which is an instance       #\n",
        "# characterized by url name, title and body                  #\n",
        "#                                                            #  \n",
        "# Contains info from a single url                            # \n",
        "#                                                            #\n",
        "##############################################################\n",
        "\n",
        "class Content:\n",
        "  def __init__(self, url, title, body):\n",
        "    self.url=url\n",
        "    self.title=title\n",
        "    self.body=body\n",
        "\n",
        "\n",
        "##############################################################\n",
        "# Defining functions to scrape                               #\n",
        "#  argument= URL                                             #\n",
        "#                                                            #\n",
        "# getPage()                                                  # \n",
        "# scrapeNYTimes()                                            #\n",
        "# scrapeBrookings()                                          #\n",
        "##############################################################\n",
        "\n",
        "\n",
        "###############################################################\n",
        "# This function gets page url                                 # \n",
        "#  and returns a beautifulsoup object as an html              #                                                     \n",
        "###############################################################\n",
        "\n",
        "def getPage(url):\n",
        "  # request page #\n",
        "  req = requests.get(url)\n",
        "\n",
        "  # return a beautifulsoup object (html source page) #\n",
        "  return BeautifulSoup(req.text, 'html.parser')\n",
        "\n",
        "\n",
        "###############################################################\n",
        "# This function request url, founds website title, text lines #\n",
        "# and   lines body and returns a Content class                #                                                     \n",
        "###############################################################\n",
        "\n",
        "def scrapeNYTimes(url):\n",
        "\n",
        "  # request page #\n",
        "  bs=getPage(url)\n",
        "\n",
        "  # extract url title #\n",
        "  title=bs.find('h1').text\n",
        "\n",
        "  # extract lines #\n",
        "  lines=bs.select('div.StoryBodyCompanionColumn div p')\n",
        "\n",
        "  # extract set of lines as a report body #\n",
        "  body='/n'.join([line.text for line in lines])\n",
        "\n",
        "  # return Content instance #\n",
        "  return Content(url, title, body)\n",
        "\n",
        "\n",
        "#################################################################\n",
        "# This function request a page, gets the page title,            #\n",
        "# the page body and returns a content class                     #\n",
        "#################################################################\n",
        "\n",
        "def scrapeBrookings(url):\n",
        "\n",
        "  # request page #\n",
        "  bs=getPage(url)\n",
        "\n",
        "  # extract page title #\n",
        "  title=bs.find('h1').text\n",
        "\n",
        "  # extract page body #\n",
        "  body=bs.find('div',{'class', 'post-body'}).text\n",
        "\n",
        "  # return Content instance #\n",
        "  return Content(url, title, body)\n",
        "\n",
        "##################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUUIvy3bfxmL"
      },
      "source": [
        "#Scrape Websites Module#\n",
        "\n",
        "*Objective: In this module we apply Functions \n",
        "           Module to NYT and Brookings urls*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CstW0yW1bJzu"
      },
      "source": [
        "#####################################\n",
        "# Brookings URL                     #\n",
        "#                                   #\n",
        "#                                   #\n",
        "#####################################\n",
        "\n",
        "###### defining url #######\n",
        "url='https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/'\n",
        "\n",
        "###### get content instance (url, title and body) ######\n",
        "content=scrapeBrookings(url)\n",
        "\n",
        "###### print content instance characteristic: title ####\n",
        "print('Title: {}'.format(content.title))\n",
        "\n",
        "print('Title printed')\n",
        "###### print content instance characteristic: url ######\n",
        "print('URL: {}\\n'.format(content.url))\n",
        "\n",
        "print('url printed')\n",
        "###### print content instance characteristic: body ####\n",
        "print(content.body)\n",
        "\n",
        "print('body printed')\n",
        "\n",
        "#####################################\n",
        "# NYT URL                           #   \n",
        "#                                   #\n",
        "#                                   #\n",
        "#####################################\n",
        "url='https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html'\n",
        "\n",
        "content=scrapeNYTimes(url)\n",
        "print('Title: {}'.format(content.title))\n",
        "print('URL: {}\\n'.format(content.url))\n",
        "print(content.body)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN_lZzrGyBW6"
      },
      "source": [
        "To make things more convenient, rather than dealing with all of these\n",
        "tag arguments and key/value pairs, you can use `BeautifulSoup` `select`\n",
        "function with a single string CSS selector for each piece of information\n",
        "you want to collect and put all of these selectors in a dictionary object\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkTYqtSMsYUp"
      },
      "source": [
        "##############################################\n",
        "#  Defining classes instances                #  \n",
        "#                                            #\n",
        "##############################################\n",
        "\n",
        "\n",
        "class Content:\n",
        "  \"\"\"\n",
        "  Common base class for all articles/pages\n",
        "  \"\"\"\n",
        "  def __init__(self, url, title, body):\n",
        "    self.url=url\n",
        "    self.title=title\n",
        "    self.body=body\n",
        "\n",
        "  def print(self):\n",
        "    \"\"\"\n",
        "    Flexible printing function controls output\n",
        "\n",
        "    \"\"\"\n",
        "    print('URL: {}'.format(self.url))\n",
        "    print('TITLE: {}'.format(self.title))\n",
        "    print('BODY: {}'.format(self.body))\n",
        "\n",
        "\n",
        "##### collects information about how to collect data ####\n",
        "##### the info here pertains to an entire website #######\n",
        "\n",
        "class Website:\n",
        "  \"\"\"\n",
        "  Contains information about website structure\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, name, url, titleTag, bodyTag):\n",
        "    self.name=name\n",
        "    self.url=url\n",
        "    self.titleTag=titleTag # stores the string tag h1 that indicates where the titles can be gound\n",
        "    self.bodyTag=bodyTag\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F_E-G98CQvu"
      },
      "source": [
        "Writing a `Crawler` to scrape the title and content of any URL that is provided\n",
        "for a given web page from a given website."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW-zl1tjCaPc"
      },
      "source": [
        "class Crawler:\n",
        "\n",
        "    def getPage(self, url):\n",
        "        try:\n",
        "            req = requests.get(url)\n",
        "        except requests.exceptions.RequestException:\n",
        "            return None\n",
        "        return BeautifulSoup(req.text, 'html.parser')\n",
        "\n",
        "    def safeGet(self, pageObj, selector):\n",
        "        \"\"\"\n",
        "        Utilty function used to get a content string from a Beautiful Soup\n",
        "        object and a selector. Returns an empty string if no object\n",
        "        is found for the given selector\n",
        "        \"\"\"\n",
        "        selectedElems = pageObj.select(selector)\n",
        "        if selectedElems is not None and len(selectedElems) > 0:\n",
        "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
        "        return ''\n",
        "\n",
        "    def parse(self, site, url):\n",
        "        \"\"\"\n",
        "        Extract content from a given page URL\n",
        "        \"\"\"\n",
        "        bs = self.getPage(url)\n",
        "        if bs is not None:\n",
        "            title = self.safeGet(bs, site.titleTag)\n",
        "            body = self.safeGet(bs, site.bodyTag)\n",
        "            if title != '' and body != '':\n",
        "                content = Content(url, title, body)\n",
        "                content.print()\n",
        "\n",
        "##################### initializing ###################\n",
        "\n",
        "crawler=Crawler()\n",
        "\n",
        "\n",
        "websites=[]\n",
        "\n",
        "for row in siteData:\n",
        "    websites.append(Website(row[0], row[1], row[2], row[3]))\n",
        "\n",
        "\n",
        "crawler.parse(websites[0], 'http://shop.oreilly.com/product/0636920028154.do')\n",
        "crawler.parse(\n",
        "    websites[1], 'http://www.reuters.com/article/us-usa-epa-pruitt-idUSKBN19W2D0')\n",
        "crawler.parse(\n",
        "    websites[2],\n",
        "    'https://www.brookings.edu/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/')\n",
        "crawler.parse(\n",
        "    websites[3], \n",
        "    'https://www.nytimes.com/2018/01/28/business/energy-environment/oil-boom.html')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZjxWQ74PgN7"
      },
      "source": [
        "# Structuring Crawlers #\n",
        "\n",
        "\n",
        "\n",
        "*   Crawling Sites Through Search\n",
        "*   Crawling Sites Through Links\n",
        "*   Crawling Multiple Page Types\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E8pMXy1RALC"
      },
      "source": [
        "# Crawling Sites Through Search #\n",
        "\n",
        "* Most sites retrieve a list of search results for a particular\n",
        "  topic by passing that topic as a string through a parameter\n",
        "  in the URL.\n",
        "\n",
        "  For example: `http://....//search=MyTopic`\n",
        "\n",
        "  The first part of this URL can be saved as a property of the `Website` object, and the topic can simply be appended to it.\n",
        "\n",
        "* After you've located and normalized the URL's on the search page. The `Content` class is much the same as in the previous examples. You are adding the URL property to keep track of where the content was found:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "For `Content` class we are adding the URL property to keep track of where the content was found.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp74rgIcPjVj"
      },
      "source": [
        "class Content:\n",
        "  \"\"\" Common base class for all article/pages\"\"\"\n",
        "  def __init___(self, topic, url, title, body):\n",
        "        self.topic=topic\n",
        "        self.title=title\n",
        "        self.body=body\n",
        "        self.url=url\n",
        "  \n",
        "  def print(self):\n",
        "    \"\"\"\n",
        "    Flexible printing function controls output\n",
        "    \"\"\"\n",
        "    print('New Article found for topic: {}'.format(self.topic))\n",
        "    print('URL: {}'.format(self.url))\n",
        "    print('TITLE: {}'.format(self.title))\n",
        "    print('BODY:\\n{}'.format(self.body))\n",
        "\n",
        "\n",
        "# the searchUrl  defines where you should go get search results\n",
        "# if you append the topic you are looking for.\n",
        "\n",
        "# the resultListing defines the \"box\" that holds information about each result\n",
        "\n",
        "# resultUrl defines the tag inside this box that will give you the exact URL \n",
        "# for the result\n",
        "\n",
        "# the absoluteUrl property is a boolean that tells you wheter these search\n",
        "# results are absolute or relative URLs\n",
        "\n",
        "\n",
        "class Website:\n",
        "  \"\"\" Contains information about website structure \"\"\"\n",
        "\n",
        "  def __init___(self, name, url, searchUrl, resultListing, resultUrl, absoluteUrl, titleTag, bodyTag):\n",
        "    self.name=name\n",
        "    self.url=url\n",
        "    self.searchUrl=searchUrl\n",
        "    self.resultListing=resultListing\n",
        "    self.absoluteUrl=absoluteUrl\n",
        "    self.titleTag=titleTag\n",
        "    self.bodyTag=bodyTag\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xupm9SvFZq4K"
      },
      "source": [
        "class Crawler:\n",
        "  def getPage(self, url):\n",
        "    try:\n",
        "      req = requests.get(url)\n",
        "    except requests.exceptions.RequestException:\n",
        "      return None\n",
        "    return BeautifulSoup(req.text, 'html.parser')\n",
        "\n",
        "   def safeGet(self, pageObj, selector):\n",
        "     childObj = pageObj.select(selector)\n",
        "     if childObj is not None and len(childObj) > 0:\n",
        "       reutrn childObj[0].get_text()\n",
        "     return ''\n",
        "\n",
        "   def search(self, topic, site):\n",
        "     \"\"\"\n",
        "     Searches a given website for a given topic and records\n",
        "     all pages found \n",
        "\n",
        "     \"\"\"\n",
        "     bs=self.getPage(site.searchUrl + topic)\n",
        "     searchResults = bs.select(site.resultListing)\n",
        "     for result in searchResults:\n",
        "       url=result.select(site.resultUrl)[0].attrs['href']\n",
        "       # Check to see wheter it's a relative or an absolute URL\n",
        "       if(site.absoluteUrl):\n",
        "         bs = self.getPage(url)\n",
        "       else:\n",
        "         bs = self.getPage(site.url+url)\n",
        "       if bs is None:\n",
        "         print('Something was wrong with that page or URL. Skipping!')\n",
        "\n",
        "          return\n",
        "       title=self.safeGet(bs, site.titleTag)\n",
        "       body=self.safeGet(bs, site.bodyTag)\n",
        "       if title!='' and body != '':\n",
        "         content = Content(topic, title, body, url)\n",
        "         content.print()\n",
        "\n",
        "crawler=Crawler()\n",
        "\n",
        "siteData = [\n",
        "    ['O\\'Reilly Media', 'http://oreilly.com', 'https://ssearch.oreilly.com/?q=',\n",
        "        'article.product-result', 'p.title a', True, 'h1', 'section#product-description'],\n",
        "    ['Reuters', 'http://reuters.com', 'http://www.reuters.com/search/news?blob=', 'div.search-result-content',\n",
        "        'h3.search-result-title a', False, 'h1', 'div.StandardArticleBody_body_1gnLA'],\n",
        "    ['Brookings', 'http://www.brookings.edu', 'https://www.brookings.edu/search/?s=',\n",
        "        'div.list-content article', 'h4.title a', True, 'h1', 'div.post-body']\n",
        "]\n",
        "\n",
        "\n",
        "sites = []\n",
        "for row in siteData:\n",
        "    sites.append(Website(row[0], row[1], row[2],\n",
        "                         row[3], row[4], row[5], row[6], row[7]))\n",
        "\n",
        "topics = ['python', 'data science']\n",
        "for topic in topics:\n",
        "    print('GETTING INFO ABOUT: ' + topic)\n",
        "    for targetSite in sites:\n",
        "        crawler.search(topic, targetSite)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}